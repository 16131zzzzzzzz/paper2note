{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "import re\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chatllm = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=\"EMPTY\", openai_api_base=\"http://localhost:8000/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strict_text(system_prompt, user_prompt, output_format, delimiter = '###', num_tries = 3, verbose = False):\n",
    "    ''' Ensures that OpenAI will always adhere to the desired output json format. \n",
    "    Uses rule-based iterative feedback to ask GPT to self-correct.\n",
    "    Keeps trying up to num_tries it it does not. Returns empty json if unable to after num_tries iterations.'''\n",
    "\n",
    "    # start off with no error message\n",
    "    error_msg = ''\n",
    "    \n",
    "    for i in range(num_tries):\n",
    "        \n",
    "        # make the output format keys with a unique identifier\n",
    "        new_output_format = {}\n",
    "        for key in output_format.keys():\n",
    "            new_output_format[f'{delimiter}{key}{delimiter}'] = output_format[key]\n",
    "        output_format_prompt = f'''\\nYou are to output the following in json format: {new_output_format}\n",
    "You must use \"{delimiter}{{key}}{delimiter}\" to enclose the each {{key}}.\n",
    "Don't return anything else except the json!!!'''\n",
    "\n",
    "        response = chatllm.invoke(\n",
    "            [\n",
    "              SystemMessage(content = system_prompt + output_format_prompt + error_msg),\n",
    "              HumanMessage(content = str(user_prompt)),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # res = response['choices'][0]['message']['content']\n",
    "        res = response.content\n",
    "\n",
    "        if verbose:\n",
    "            print('System prompt:', system_prompt + output_format_prompt + error_msg)\n",
    "            print('\\nUser prompt:', str(user_prompt))\n",
    "            print('\\nGPT response:', res)\n",
    "        \n",
    "        # try-catch block to ensure output format is adhered to\n",
    "        try:\n",
    "            # check key appears for each element in the output\n",
    "            for key in new_output_format.keys():\n",
    "                # if output field missing, raise an error\n",
    "                if key not in res: raise Exception(f\"{key} not in json output\")\n",
    "                \n",
    "            # if all is good, we then extract out the fields\n",
    "            # Use regular expressions to extract keys and values\n",
    "            pattern = fr\",*\\s*['|\\\"]{delimiter}([^#]*){delimiter}['|\\\"]: \"\n",
    "\n",
    "            matches = re.split(pattern, res[1:-1])\n",
    "\n",
    "            # remove null matches\n",
    "            my_matches = [match for match in matches if match !='']\n",
    "\n",
    "            # remove the ' or \" from the value matches\n",
    "            curated_matches = [match[1:-1] if match[0] in '\\'\"' else match for match in my_matches]\n",
    "\n",
    "            # create a dictionary\n",
    "            end_dict = {}\n",
    "            for i in range(0, len(curated_matches), 2):\n",
    "                end_dict[curated_matches[i]] = curated_matches[i+1]\n",
    "\n",
    "            return end_dict\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"\\n\\nResult: {res}\\n\\nError message: {str(e)}\\nYou must use \\\"{delimiter}{{key}}{delimiter}\\\" to enclose the each {{key}}.\"\n",
    "            print(\"An exception occurred:\", str(e))\n",
    "            print(\"Current invalid json format:\", res)\n",
    "         \n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'Positive', 'Tense': 'Present'}\n"
     ]
    }
   ],
   "source": [
    "res = strict_text(system_prompt = 'You are a classifier',\n",
    "                    user_prompt = 'It is a beautiful day',\n",
    "                    output_format = {\"Sentiment\": \"Type of Sentiment\",\n",
    "                                    \"Tense\": \"Type of Tense\"})\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"What is the main idea of this paper?\",\n",
    "             \"What is the definition of the problem?\",\n",
    "             \"What are the difficulties of the problem?\",\n",
    "             \"What the papers in the past do on the problem?\",\n",
    "             \"What downsides the past papers have?\",\n",
    "             \"Summarize the main steps of the method and the advantage of the steps.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = [\n",
    "  \"1 Introduction\",\n",
    "  \"2 Problem Definition\",\n",
    "  \"3 Framework Of Search\",\n",
    "  \"4 Top-K Star Matching\",\n",
    "  \"5 Top-K Join & Pattern Decomposition\",\n",
    "  \"6 Evaluation\",\n",
    "  \"7 Related Work\",\n",
    "  \"8 Conclusion\",\n",
    "  \"Acknowledgments\",\n",
    "  \"References\"\n",
    "]\n",
    "\n",
    "section_summaries = [\n",
    "  \"The text discusses the use of graphs to represent real-life data, with an emphasis on multi-modal graphs that contain both structured and unstructured data. The authors propose a new graph data model called the neural-symbolic graph database (NSGD) that extends property graph models with content and structural embeddings in every node and edge. They also introduce a novel algorithm called NSMatch for subgraph search over NSGDs, which uses sophisticated ranking functions to generate top answers in a monotonic decreasing order of matching score. The authors provide experimental results that demonstrate the effectiveness and efficiency of their algorithms.\",\n",
    "  \"The text discusses the problem of Neural-Symbolic Subgraph Matching (NSMatch) in Neural-Symbolic Graph Databases (NSGDs). NSGDs are directed labeled graphs that consist of nodes, edges, labels, attributes, and vectors representing unstructured data. NSMatch aims to find a subgraph match in an NSGD based on a given graph pattern and a similarity function. The problem is to find the top-k subgraph matches in the NSGD that meet a certain matching score. NSMatch is NP-hard due to its special case being NP-complete.\",\n",
    "  \"The text discusses an algorithmic framework for attacking the hard problem of NSMatch. The framework consists of three main steps: pattern decomposition, star matching, and top-k join.\",\n",
    "  \"The text discusses the SMat algorithm for computing top-k star matches in a star pattern. The algorithm consists of three phases: Phase 1 identifies candidate node matches and top-h leaf matches; Phase 2 selects the best k star matches to form a pseudo top-k star matches and sorts the leaf matches; and Phase 3 maintains the priority queue and pops the best match until the desired top-k matches are obtained. The algorithm uses the content vectors Csq and Cg, as well as the structural vector Sg, to identify edges and compute the similarity function δ(·) between Csq and Cg. The value of h, which determines the number of leaf matches to be scanned, is pre-determined in the system. The algorithm is efficient and can accurately complete the missing edges of top-k star matches.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         5 Top-K Join & Pattern Decomposition\",\n",
    "  \"\",\n",
    "  \"\",\n",
    "  \"The text discusses the related work on subgraph search and approximate nearest neighbors search (ANNS) for graph-based methods. It categorizes the subgraph search methods into symbolic methods and neural approaches, and provides examples of each category. It also focuses on graph-based ANNS methods, particularly HNSW and NSG, and explains how NSMatch differs from traditional subgraph matching and existing ANNS algorithms.\",\n",
    "  \"The text discusses a new method called neural-symbolic subgraph matching (NSMatch) that uses a neuralsymbolic graph database (NSGD) model to support applications of multi-modal knowledge graphs and multi-modal social medias. The method includes a general and efficient algorithmic framework to process NSMatch, strategies of edge deletion to speed up graph-based ANNS, and a neural-symbolic learning model to complete the missing edges of the NSGD. The text also mentions future work, such as studying incremental NSMatch and investigating more pattern types over large NSGDs.\",\n",
    "  \"The text acknowledges the financial support provided by various organizations to the authors Ye Yuan and Jianbin Qin for their research on the National Key R&D Program of China, the NSFC, and the DITDP.\",\n",
    "  \"The text discusses various approaches and techniques for efficient and accurate subgraph matching, similarity search, and knowledge graph completion. It covers techniques such as using approximate nearest neighbor queries, translating embeddings, and graph-based methods. The paper also discusses the use of knowledge graphs for text-centric information retrieval and efficient k-nearest neighbor graph construction.\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################\n",
      "@@@prompt:@@@\n",
      "For answering the question: What is the main idea of this paper?, you need to choose which 1-2 sections to read and return the indexes of the section titles you want to read. The section titles are as follows:\n",
      "0  ##1 Introduction##\n",
      "1  ##2 Problem Definition##\n",
      "2  ##3 Framework Of Search##\n",
      "3  ##4 Top-K Star Matching##\n",
      "4  ##5 Top-K Join & Pattern Decomposition##\n",
      "5  ##6 Evaluation##\n",
      "6  ##7 Related Work##\n",
      "7  ##8 Conclusion##\n",
      "8  ##Acknowledgments##\n",
      "9  ##References##\n",
      "\n",
      "@@@res@@@\n",
      "{'indexes': '[\"2\", \"4\", \"5\", \"6\"]'}\n",
      "###########################\n",
      "@@@prompt:@@@\n",
      "For answering the question: What is the definition of the problem?, you need to choose which 1-2 sections to read and return the indexes of the section titles you want to read. The section titles are as follows:\n",
      "0  ##1 Introduction##\n",
      "1  ##2 Problem Definition##\n",
      "2  ##3 Framework Of Search##\n",
      "3  ##4 Top-K Star Matching##\n",
      "4  ##5 Top-K Join & Pattern Decomposition##\n",
      "5  ##6 Evaluation##\n",
      "6  ##7 Related Work##\n",
      "7  ##8 Conclusion##\n",
      "8  ##Acknowledgments##\n",
      "9  ##References##\n",
      "\n",
      "@@@res@@@\n",
      "{'indexes': \"['2', '4', '5', '6', '7', '8']\"}\n",
      "###########################\n",
      "@@@prompt:@@@\n",
      "For answering the question: What are the difficulties of the problem?, you need to choose which 1-2 sections to read and return the indexes of the section titles you want to read. The section titles are as follows:\n",
      "0  ##1 Introduction##\n",
      "1  ##2 Problem Definition##\n",
      "2  ##3 Framework Of Search##\n",
      "3  ##4 Top-K Star Matching##\n",
      "4  ##5 Top-K Join & Pattern Decomposition##\n",
      "5  ##6 Evaluation##\n",
      "6  ##7 Related Work##\n",
      "7  ##8 Conclusion##\n",
      "8  ##Acknowledgments##\n",
      "9  ##References##\n",
      "\n",
      "@@@res@@@\n",
      "{'indexes': '[\\n\"##2 Problem Definition##\",\\n\"##4 Top-K Star Matching##\"\\n]\\n'}\n",
      "###########################\n",
      "@@@prompt:@@@\n",
      "For answering the question: What the papers in the past do on the problem?, you need to choose which 1-2 sections to read and return the indexes of the section titles you want to read. The section titles are as follows:\n",
      "0  ##1 Introduction##\n",
      "1  ##2 Problem Definition##\n",
      "2  ##3 Framework Of Search##\n",
      "3  ##4 Top-K Star Matching##\n",
      "4  ##5 Top-K Join & Pattern Decomposition##\n",
      "5  ##6 Evaluation##\n",
      "6  ##7 Related Work##\n",
      "7  ##8 Conclusion##\n",
      "8  ##Acknowledgments##\n",
      "9  ##References##\n",
      "\n",
      "@@@res@@@\n",
      "{'indexes': '[\\n\"##2 Problem Definition##\",\\n\"##7 Related Work##\"\\n]\\n'}\n",
      "###########################\n",
      "@@@prompt:@@@\n",
      "For answering the question: What downsides the past papers have?, you need to choose which 1-2 sections to read and return the indexes of the section titles you want to read. The section titles are as follows:\n",
      "0  ##1 Introduction##\n",
      "1  ##2 Problem Definition##\n",
      "2  ##3 Framework Of Search##\n",
      "3  ##4 Top-K Star Matching##\n",
      "4  ##5 Top-K Join & Pattern Decomposition##\n",
      "5  ##6 Evaluation##\n",
      "6  ##7 Related Work##\n",
      "7  ##8 Conclusion##\n",
      "8  ##Acknowledgments##\n",
      "9  ##References##\n",
      "\n",
      "@@@res@@@\n",
      "{'indexes': \"['2', '6']\"}\n",
      "###########################\n",
      "@@@prompt:@@@\n",
      "For answering the question: Summarize the main steps of the method and the advantage of the steps., you need to choose which 1-2 sections to read and return the indexes of the section titles you want to read. The section titles are as follows:\n",
      "0  ##1 Introduction##\n",
      "1  ##2 Problem Definition##\n",
      "2  ##3 Framework Of Search##\n",
      "3  ##4 Top-K Star Matching##\n",
      "4  ##5 Top-K Join & Pattern Decomposition##\n",
      "5  ##6 Evaluation##\n",
      "6  ##7 Related Work##\n",
      "7  ##8 Conclusion##\n",
      "8  ##Acknowledgments##\n",
      "9  ##References##\n",
      "\n",
      "@@@res@@@\n",
      "{'indexes': \"['2', '5']\"}\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "  print(\"###########################\")\n",
    "  prompt = f\"For answering the question: {question}, you need to choose which 1-2 sections to read and return the indexes of the section titles you want to read. The section titles are as follows:\\n\"\n",
    "  for i, section_title in enumerate(sections):\n",
    "    prompt += str(i)\n",
    "    prompt += f\"  ##{section_title}##\\n\"\n",
    "\n",
    "  res = strict_text(system_prompt = 'You are a paper reader',\n",
    "                    user_prompt = prompt,\n",
    "                    output_format = {\"indexes\": [\"the indexes of the section titles you want to read, only include 1-2 of them\"]})\n",
    "  \n",
    "  print(\"@@@prompt:@@@\")\n",
    "  print(prompt)\n",
    "  print(\"@@@res@@@\")\n",
    "  print(res)\n",
    "  print(\"@@@choosed sections@@@\")\n",
    "\n",
    "\n",
    "  for index in res['indexes']:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indexes': \"['2', '4', '6']\"}\n"
     ]
    }
   ],
   "source": [
    "res = strict_text(system_prompt = 'You are a paper reader',\n",
    "                    user_prompt = prompt,\n",
    "                    output_format = {\"indexes\": [\"the indexes of the section titles you want to read, only include 1-2 of them\"]})\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonformer import Jsonformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Got unknown type G",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 18\u001b[0m\n\u001b[1;32m      4\u001b[0m json_schema \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     }\n\u001b[1;32m     15\u001b[0m }\n\u001b[1;32m     17\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerate a person\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms information based on the following schema:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m jsonformer \u001b[38;5;241m=\u001b[39m Jsonformer(model, tokenizer, json_schema, prompt)\n\u001b[1;32m     19\u001b[0m generated_data \u001b[38;5;241m=\u001b[39m jsonformer()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_data)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jsonformer/main.py:36\u001b[0m, in \u001b[0;36mJsonformer.__init__\u001b[0;34m(self, model, tokenizer, json_schema, prompt, debug, max_array_length, max_number_tokens, temperature, max_string_token_length)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjson_schema \u001b[38;5;241m=\u001b[39m json_schema\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt \u001b[38;5;241m=\u001b[39m prompt\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumber_logit_processor \u001b[38;5;241m=\u001b[39m OutputNumbersTokens(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_marker \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|GENERATION|\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug_on \u001b[38;5;241m=\u001b[39m debug\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jsonformer/logits_processors.py:67\u001b[0m, in \u001b[0;36mOutputNumbersTokens.__init__\u001b[0;34m(self, tokenizer, prompt)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokenizer: PreTrainedTokenizer, prompt: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenized_prompt \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m     vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallowed_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(vocab_size, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/chat_models/base.py:608\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    603\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    607\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 608\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    609\u001b[0m         [messages], stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    610\u001b[0m     )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/chat_models/base.py:359\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    358\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 359\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    360\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    361\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    363\u001b[0m ]\n\u001b[1;32m    364\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/chat_models/base.py:349\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 349\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    350\u001b[0m                 m,\n\u001b[1;32m    351\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    352\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    353\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    354\u001b[0m             )\n\u001b[1;32m    355\u001b[0m         )\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/chat_models/base.py:501\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    502\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    503\u001b[0m     )\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/chat_models/openai.py:358\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     stream_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[1;32m    355\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    356\u001b[0m     )\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _generate_from_stream(stream_iter)\n\u001b[0;32m--> 358\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    359\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    360\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_with_retry(\n\u001b[1;32m    361\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessage_dicts, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    362\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/chat_models/openai.py:373\u001b[0m, in \u001b[0;36mChatOpenAI._create_message_dicts\u001b[0;34m(self, messages, stop)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`stop` found in both the input and default params.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    372\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m stop\n\u001b[0;32m--> 373\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m [convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m message_dicts, params\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/chat_models/openai.py:373\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`stop` found in both the input and default params.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    372\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m stop\n\u001b[0;32m--> 373\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m [convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m message_dicts, params\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/adapters/openai.py:84\u001b[0m, in \u001b[0;36mconvert_message_to_dict\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m     78\u001b[0m     message_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: message\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: message\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m     82\u001b[0m     }\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unknown type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m message\u001b[38;5;241m.\u001b[39madditional_kwargs:\n\u001b[1;32m     86\u001b[0m     message_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39madditional_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: Got unknown type G"
     ]
    }
   ],
   "source": [
    "model = chatllm\n",
    "tokenizer = chatllm\n",
    "\n",
    "json_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"string\"},\n",
    "        \"age\": {\"type\": \"number\"},\n",
    "        \"is_student\": {\"type\": \"boolean\"},\n",
    "        \"courses\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "prompt = \"Generate a person's information based on the following schema:\"\n",
    "jsonformer = Jsonformer(model, tokenizer, json_schema, prompt)\n",
    "generated_data = jsonformer()\n",
    "\n",
    "print(generated_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
